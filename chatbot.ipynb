{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyNa6hZXdlFj9tlpa3DBAeKw",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/afzal/MiniTalky/blob/main/chatbot.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "irT2vHDMESzP",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "2cbac8e2-e006-46c3-dede-590d740a0d99"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Device set to use cpu\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "GPT2LMHeadModel(\n",
            "  (transformer): GPT2Model(\n",
            "    (wte): Embedding(50257, 768)\n",
            "    (wpe): Embedding(1024, 768)\n",
            "    (drop): Dropout(p=0.1, inplace=False)\n",
            "    (h): ModuleList(\n",
            "      (0-5): 6 x GPT2Block(\n",
            "        (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
            "        (attn): GPT2Attention(\n",
            "          (c_attn): Conv1D(nf=2304, nx=768)\n",
            "          (c_proj): Conv1D(nf=768, nx=768)\n",
            "          (attn_dropout): Dropout(p=0.1, inplace=False)\n",
            "          (resid_dropout): Dropout(p=0.1, inplace=False)\n",
            "        )\n",
            "        (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
            "        (mlp): GPT2MLP(\n",
            "          (c_fc): Conv1D(nf=3072, nx=768)\n",
            "          (c_proj): Conv1D(nf=768, nx=3072)\n",
            "          (act): NewGELUActivation()\n",
            "          (dropout): Dropout(p=0.1, inplace=False)\n",
            "        )\n",
            "      )\n",
            "    )\n",
            "    (ln_f): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
            "  )\n",
            "  (lm_head): Linear(in_features=768, out_features=50257, bias=False)\n",
            ")\n"
          ]
        }
      ],
      "source": [
        "from transformers import pipeline\n",
        "\n",
        "# Load the model\n",
        "model = pipeline(\"text-generation\", model=\"distilgpt2\")\n",
        "print(model.model) #This shows the raw model inside the pipeline—the actual distilgpt2 architecture.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "asks gpt2 Can’t Really Do\n",
        "\n",
        "    These are in the pipeline options, but gpt2 isn’t trained for them, so they won’t work well:\n",
        "        \"summarization\", \"question-answering\", \"translation\", \"sentiment-analysis\"\n",
        "            Why? gpt2 generates text forward—it doesn’t understand context backward or analyze text like other models (e.g., BERT or BART).\n",
        "            Example: If you try pipeline(\"summarization\", model=\"gpt2\"), it’ll fail or give nonsense because gpt2 isn’t built for that."
      ],
      "metadata": {
        "id": "L5jI8JYOKATI"
      }
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "lfpDtLQCKCCb"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Chat loop\n",
        "while True:\n",
        "    prompt = input(\"You: \")\n",
        "    result = model(prompt, max_length=50, temperature=0.7, top_k=50)\n",
        "    print(\"Bot:\", result[0][\"generated_text\"])"
      ],
      "metadata": {
        "id": "XgXcUnk2Dpxc",
        "outputId": "538da4e1-3e31-48d9-cc89-1efa801c5e35",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "execution_count": 3,
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "You: Hi\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Bot: Hi. As a result, I can’t believe that I‬ve got to the end of this year,’ I think I‬ll be able to finish that year in an even better place.\n",
            "\n",
            "Advertisements\n",
            "You: how are you?\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Bot: how are you?\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "You: what day is today?\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Bot: what day is today?›\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "Interrupted by user",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-3-e027f18b75ea>\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m# Chat loop\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;32mwhile\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m     \u001b[0mprompt\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"You: \"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      4\u001b[0m     \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mprompt\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmax_length\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m50\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtemperature\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0.7\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtop_k\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m50\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Bot:\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"generated_text\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/ipykernel/kernelbase.py\u001b[0m in \u001b[0;36mraw_input\u001b[0;34m(self, prompt)\u001b[0m\n\u001b[1;32m   1175\u001b[0m                 \u001b[0;34m\"raw_input was called, but this frontend does not support input requests.\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1176\u001b[0m             )\n\u001b[0;32m-> 1177\u001b[0;31m         return self._input_request(\n\u001b[0m\u001b[1;32m   1178\u001b[0m             \u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mprompt\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1179\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_parent_ident\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"shell\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/ipykernel/kernelbase.py\u001b[0m in \u001b[0;36m_input_request\u001b[0;34m(self, prompt, ident, parent, password)\u001b[0m\n\u001b[1;32m   1217\u001b[0m             \u001b[0;32mexcept\u001b[0m \u001b[0mKeyboardInterrupt\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1218\u001b[0m                 \u001b[0;31m# re-raise KeyboardInterrupt, to truncate traceback\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1219\u001b[0;31m                 \u001b[0;32mraise\u001b[0m \u001b[0mKeyboardInterrupt\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Interrupted by user\"\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1220\u001b[0m             \u001b[0;32mexcept\u001b[0m \u001b[0mException\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1221\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlog\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwarning\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Invalid Message:\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mexc_info\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: Interrupted by user"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "\n",
        "# Chat loop\n",
        "while True:\n",
        "    prompt = input(\"You: \")\n",
        "    result = model(prompt, max_length=50, temperature=0.7, top_k=40, truncation=True)\n",
        "    # Remove the prompt from the reply\n",
        "    reply = result[0][\"generated_text\"].replace(prompt, \"\").strip()\n",
        "    print(\"Bot:\", reply)"
      ],
      "metadata": {
        "id": "hZXthza4FwpL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#**truncation=True**\n",
        "\n",
        "**truncation=True** is a parameter you might see when working with the transformers library (like in our chatbot code). Let me explain it in very small, simple steps!\n",
        "What is truncation=True?\n",
        "\n",
        "    It’s an option you can add when using a pipeline in transformers.\n",
        "    It tells the model how to handle text that’s too long for it to process.\n",
        "\n",
        "Step-by-Step Explanation\n",
        "\n",
        "    Models Have Limits\n",
        "        Language models like gpt2 or distilgpt2 can only handle a certain number of words (or \"tokens\") at once.\n",
        "        For gpt2, this limit is usually 1024 tokens (a token is roughly a word or part of a word).\n",
        "    What Happens Without Truncation?\n",
        "        If your input (like a really long prompt) is more than 1024 tokens, the model crashes or throws an error.\n",
        "        Example: You type a 2000-word story as a prompt—it won’t work.\n",
        "    What truncation=True Does\n",
        "        It says: \"If the input is too long, cut it down to fit the model’s limit.\"\n",
        "        The model keeps only the first 1024 tokens (or whatever the limit is) and ignores the rest.\n",
        "    Example\n",
        "        Prompt: \"The cat sat on the mat and then ran to the door and kept running all day...\"\n",
        "        If it’s too long, with truncation=True, it might chop it to: \"The cat sat on the mat and then ran to the door.\"\n",
        "        The rest gets ignored so the model can process it.\n",
        "    Why Use It?\n",
        "        Prevents errors when you accidentally give a huge input.\n",
        "        Keeps things running smoothly.\n"
      ],
      "metadata": {
        "id": "zWWXL-1MEyKO"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **top_k=40**\n",
        "Step-by-Step: What **top_k=40** Does\n",
        "\n",
        "    Model Predicts Words\n",
        "        When you type \"Hello,\" the model looks at all possible next words and gives them probabilities:\n",
        "            \"there\" (30%)\n",
        "            \"friend\" (20%)\n",
        "            \"how\" (15%)\n",
        "            \"to\" (10%)\n",
        "            And tons more (like \"zebra\" at 0.001%).\n",
        "    Sorts the List\n",
        "        It ranks them from highest to lowest probability:\n",
        "            \"there\" (30%)\n",
        "            \"friend\" (20%)\n",
        "            \"how\" (15%) ...and so on.\n",
        "    Picks the Top 40\n",
        "        With top_k=40, it takes only the top 40 most likely words.\n",
        "        So, \"there,\" \"friend,\" \"how,\" etc., are in, but super rare ones like \"zebra\" (way down the list) are ignored.\n",
        "    Chooses Randomly from Those 40\n",
        "        It picks one word from those 40, based on their probabilities.\n",
        "        Higher probability = more likely, but it’s still a bit random.\n",
        "    Builds the Reply\n",
        "        It repeats this for each word until it hits max_length=50.\n",
        "\n",
        "Example with Your Chatbot\n",
        "\n",
        "    You type: \"You are\"\n",
        "    Model’s top probabilities might be:\n",
        "        \"a\" (40%)\n",
        "        \"so\" (20%)\n",
        "        \"very\" (15%)\n",
        "        ...down to the 40th word.\n",
        "    With top_k=40, it picks from those 40 (not the full list of thousands).\n",
        "    Possible reply: \"Bot: You are a great person to chat with today!\""
      ],
      "metadata": {
        "id": "A4qG9BfqPtGa"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **temperature=0.7**\n",
        "The Model Picks Words\n",
        "\n",
        "    When you type \"Hello,\" the model looks at possible next words: \"Hi\" (30%), \"Hey\" (20%), \"Greetings\" (10%), etc.\n",
        "    Each word has a probability (chance of being picked).\n",
        "\n",
        "Temperature Changes Choices\n",
        "\n",
        "    temperature tweaks these probabilities:\n",
        "        High temperature (e.g., 1.5): Makes all words more equal, so it might pick weird ones (like \"Hello banana\").\n",
        "        Low temperature (e.g., 0.3): Boosts likely words, ignores unlikely ones (sticks to \"Hello there\").\n",
        "        temperature=0.7: A middle ground—mostly picks sensible words but allows some variety."
      ],
      "metadata": {
        "id": "6HBJX3XWQsDw"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Chat loop\n",
        "while True:\n",
        "    prompt = input(\"You: \")\n",
        "    if not prompt.strip():  # If input is empty, skip\n",
        "        print(\"Bot: Say something!\")\n",
        "        continue\n",
        "    full_prompt = f\"Human: {prompt} Bot:\"  # Add context\n",
        "    result = model(full_prompt, max_length=50, temperature=0.9, top_k=50, truncation=True)\n",
        "    reply = result[0][\"generated_text\"].replace(full_prompt, \"\").strip()\n",
        "    print(\"Bot:\", reply)"
      ],
      "metadata": {
        "id": "tjwHgpr0CWTm"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}