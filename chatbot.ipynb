{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyOvHTAGpc0f1sK/WUS6Bg/r",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/afzal/MiniTalky/blob/main/chatbot.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "irT2vHDMESzP"
      },
      "outputs": [],
      "source": [
        "from transformers import pipeline\n",
        "\n",
        "# Load the model\n",
        "model = pipeline(\"text-generation\", model=\"distilgpt2\")\n",
        "print(model.model) #This shows the raw model inside the pipeline—the actual distilgpt2 architecture.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "asks gpt2 Can’t Really Do\n",
        "\n",
        "    These are in the pipeline options, but gpt2 isn’t trained for them, so they won’t work well:\n",
        "        \"summarization\", \"question-answering\", \"translation\", \"sentiment-analysis\"\n",
        "            Why? gpt2 generates text forward—it doesn’t understand context backward or analyze text like other models (e.g., BERT or BART).\n",
        "            Example: If you try pipeline(\"summarization\", model=\"gpt2\"), it’ll fail or give nonsense because gpt2 isn’t built for that."
      ],
      "metadata": {
        "id": "L5jI8JYOKATI"
      }
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "lfpDtLQCKCCb"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Chat loop\n",
        "while True:\n",
        "    prompt = input(\"You: \")\n",
        "    result = model(prompt, max_length=50, temperature=0.7, top_k=50)\n",
        "    print(\"Bot:\", result[0][\"generated_text\"])"
      ],
      "metadata": {
        "id": "XgXcUnk2Dpxc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "\n",
        "# Chat loop\n",
        "while True:\n",
        "    prompt = input(\"You: \")\n",
        "    result = model(prompt, max_length=50, temperature=0.7, top_k=40, truncation=True)\n",
        "    # Remove the prompt from the reply\n",
        "    reply = result[0][\"generated_text\"].replace(prompt, \"\").strip()\n",
        "    print(\"Bot:\", reply)"
      ],
      "metadata": {
        "id": "hZXthza4FwpL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#**truncation=True**\n",
        "\n",
        "**truncation=True** is a parameter you might see when working with the transformers library (like in our chatbot code). Let me explain it in very small, simple steps!\n",
        "What is truncation=True?\n",
        "\n",
        "    It’s an option you can add when using a pipeline in transformers.\n",
        "    It tells the model how to handle text that’s too long for it to process.\n",
        "\n",
        "Step-by-Step Explanation\n",
        "\n",
        "    Models Have Limits\n",
        "        Language models like gpt2 or distilgpt2 can only handle a certain number of words (or \"tokens\") at once.\n",
        "        For gpt2, this limit is usually 1024 tokens (a token is roughly a word or part of a word).\n",
        "    What Happens Without Truncation?\n",
        "        If your input (like a really long prompt) is more than 1024 tokens, the model crashes or throws an error.\n",
        "        Example: You type a 2000-word story as a prompt—it won’t work.\n",
        "    What truncation=True Does\n",
        "        It says: \"If the input is too long, cut it down to fit the model’s limit.\"\n",
        "        The model keeps only the first 1024 tokens (or whatever the limit is) and ignores the rest.\n",
        "    Example\n",
        "        Prompt: \"The cat sat on the mat and then ran to the door and kept running all day...\"\n",
        "        If it’s too long, with truncation=True, it might chop it to: \"The cat sat on the mat and then ran to the door.\"\n",
        "        The rest gets ignored so the model can process it.\n",
        "    Why Use It?\n",
        "        Prevents errors when you accidentally give a huge input.\n",
        "        Keeps things running smoothly.\n"
      ],
      "metadata": {
        "id": "zWWXL-1MEyKO"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **top_k=40**\n",
        "Step-by-Step: What **top_k=40** Does\n",
        "\n",
        "    Model Predicts Words\n",
        "        When you type \"Hello,\" the model looks at all possible next words and gives them probabilities:\n",
        "            \"there\" (30%)\n",
        "            \"friend\" (20%)\n",
        "            \"how\" (15%)\n",
        "            \"to\" (10%)\n",
        "            And tons more (like \"zebra\" at 0.001%).\n",
        "    Sorts the List\n",
        "        It ranks them from highest to lowest probability:\n",
        "            \"there\" (30%)\n",
        "            \"friend\" (20%)\n",
        "            \"how\" (15%) ...and so on.\n",
        "    Picks the Top 40\n",
        "        With top_k=40, it takes only the top 40 most likely words.\n",
        "        So, \"there,\" \"friend,\" \"how,\" etc., are in, but super rare ones like \"zebra\" (way down the list) are ignored.\n",
        "    Chooses Randomly from Those 40\n",
        "        It picks one word from those 40, based on their probabilities.\n",
        "        Higher probability = more likely, but it’s still a bit random.\n",
        "    Builds the Reply\n",
        "        It repeats this for each word until it hits max_length=50.\n",
        "\n",
        "Example with Your Chatbot\n",
        "\n",
        "    You type: \"You are\"\n",
        "    Model’s top probabilities might be:\n",
        "        \"a\" (40%)\n",
        "        \"so\" (20%)\n",
        "        \"very\" (15%)\n",
        "        ...down to the 40th word.\n",
        "    With top_k=40, it picks from those 40 (not the full list of thousands).\n",
        "    Possible reply: \"Bot: You are a great person to chat with today!\""
      ],
      "metadata": {
        "id": "A4qG9BfqPtGa"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **temperature=0.7**\n",
        "The Model Picks Words\n",
        "\n",
        "    When you type \"Hello,\" the model looks at possible next words: \"Hi\" (30%), \"Hey\" (20%), \"Greetings\" (10%), etc.\n",
        "    Each word has a probability (chance of being picked).\n",
        "\n",
        "Temperature Changes Choices\n",
        "\n",
        "    temperature tweaks these probabilities:\n",
        "        High temperature (e.g., 1.5): Makes all words more equal, so it might pick weird ones (like \"Hello banana\").\n",
        "        Low temperature (e.g., 0.3): Boosts likely words, ignores unlikely ones (sticks to \"Hello there\").\n",
        "        temperature=0.7: A middle ground—mostly picks sensible words but allows some variety."
      ],
      "metadata": {
        "id": "6HBJX3XWQsDw"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Chat loop\n",
        "while True:\n",
        "    prompt = input(\"You: \")\n",
        "    if not prompt.strip():  # If input is empty, skip\n",
        "        print(\"Bot: Say something!\")\n",
        "        continue\n",
        "    full_prompt = f\"Human: {prompt} Bot:\"  # Add context\n",
        "    result = model(full_prompt, max_length=50, temperature=0.9, top_k=50, truncation=True)\n",
        "    #print(result)\n",
        "    reply = result[0][\"generated_text\"].replace(full_prompt, \"\").strip()\n",
        "    print(\"Bot:\", reply)"
      ],
      "metadata": {
        "id": "tjwHgpr0CWTm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "while True:\n",
        "    prompt = input(\"You: \")\n",
        "    if prompt.lower() == \"quit\":\n",
        "        print(\"Bot: Bye bye!\")\n",
        "        break\n",
        "    result = model(prompt, max_length=50, temperature=0.7, top_k=50)\n",
        "    print(\"Bot:\", result[0][\"generated_text\"])"
      ],
      "metadata": {
        "id": "k45aMOuUkvyr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import pipeline\n",
        "\n",
        "# Load the model\n",
        "model = pipeline(\"text-generation\", model=\"distilgpt2\")\n",
        "\n",
        "# Chat loop\n",
        "while True:\n",
        "    prompt = input(\"You: \")\n",
        "    # Generate reply\n",
        "    result = model(prompt, max_length=50, temperature=0.7, top_k=50, truncation = True, return_full_text=False)\n",
        "    # Show reply without input\n",
        "    print(\"Bot:\", result[0][\"generated_text\"])"
      ],
      "metadata": {
        "id": "AUShigYKl7Ar"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#**return_full_text=False**\n",
        "Added return_full_text=False to the model call.\n",
        "This tells the model not to include your input in the output, so it only gives the new part."
      ],
      "metadata": {
        "id": "F8Llas8Xx6mR"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "xBUc2lDbx-sz"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}